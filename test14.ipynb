{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180284d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q keras\n",
    "\n",
    "\n",
    "## Imports libs\n",
    "import os\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d2cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fa8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d85fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d5e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import SVHN\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a2f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar=tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff5b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test  = x_test  / 255.0 \n",
    "\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de82c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.Conv2D(32,\n",
    "                                 kernel_size=(3,3),\n",
    "                                 strides=(1,1),\n",
    "                                 padding=\"same\",\n",
    "                                 activation=\"relu\",\n",
    "                                 input_shape=(32,32,3)),\n",
    "             tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Conv2D(64,\n",
    "                                 kernel_size=(3,3),\n",
    "                                 strides=(1,1),\n",
    "                                 padding=\"same\",\n",
    "                                 activation=\"relu\"),\n",
    "             tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Conv2D(128,\n",
    "                                 kernel_size=(3,3),\n",
    "                                 strides=(1,1),\n",
    "                                 padding=\"same\",\n",
    "                                 activation=\"relu\"),\n",
    "             tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.MaxPooling2D(2,2),\n",
    "             tf.keras.layers.Conv2D(256,\n",
    "                                 kernel_size=(3,3),\n",
    "                                 strides=(1,1),\n",
    "                                 padding=\"same\",\n",
    "                                 activation=\"relu\"),\n",
    "             tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.MaxPooling2D(2,2),\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "             tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dropout(0.5),\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(1028, activation=\"relu\"),\n",
    "             tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dropout(0.5),\n",
    "             tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff6d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(32, 32,3)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0cbebe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a5238ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e64a5cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 403s 258ms/step - loss: 1.0890 - accuracy: 0.6282\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 391s 250ms/step - loss: 0.8526 - accuracy: 0.7050\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 384s 246ms/step - loss: 0.6927 - accuracy: 0.7627\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 368s 235ms/step - loss: 0.5787 - accuracy: 0.8011\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 367s 235ms/step - loss: 0.4563 - accuracy: 0.8432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2208279b6a0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0847bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 15s - loss: 0.6396 - accuracy: 0.7966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6395964026451111, 0.7965999841690063]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "25cc626a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Layer batch_normalization_7:<class 'keras.layers.normalization.batch_normalization.BatchNormalization'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-aed24053c174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mquant_aware_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mquant_aware_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36mquantize_model\u001b[1;34m(to_quantize)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[0mannotated_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantize_annotate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_quantize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mquantize_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotated_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\metrics.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FAILURE_LABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\metrics.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUCCESS_LABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36mquantize_apply\u001b[1;34m(model, scheme)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m   return keras.models.clone_model(\n\u001b[1;32m--> 480\u001b[1;33m       transformed_model, input_tensors=None, clone_function=_quantize)\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mclone_model\u001b[1;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m       return _clone_sequential_model(\n\u001b[1;32m--> 449\u001b[1;33m           model, input_tensors=input_tensors, layer_fn=clone_function)\n\u001b[0m\u001b[0;32m    450\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m       return _clone_functional_model(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m_clone_sequential_model\u001b[1;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[0;32m    324\u001b[0m     cloned_layer = (\n\u001b[0;32m    325\u001b[0m         \u001b[0m_clone_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         if isinstance(layer, InputLayer) else layer_fn(layer))\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloned_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[0mlayer_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloned_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36m_quantize\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    429\u001b[0m       raise RuntimeError(\n\u001b[0;32m    430\u001b[0m           error_msg.format(layer.name, layer.__class__,\n\u001b[1;32m--> 431\u001b[1;33m                            quantize_registry.__class__))\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;31m# `QuantizeWrapper` does not copy any additional layer params from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Layer batch_normalization_7:<class 'keras.layers.normalization.batch_normalization.BatchNormalization'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API."
     ]
    }
   ],
   "source": [
    "base_model=model\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fff08bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ebe48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class FixedRangeQuantizer(tfmot.quantization.keras.quantizers.Quantizer):\n",
    "  \"\"\"Quantizer which forces outputs to be between -1 and 1.\"\"\"\n",
    "\n",
    "  def build(self, tensor_shape, name, layer):\n",
    "    # Not needed. No new TensorFlow variables needed.\n",
    "    return {}\n",
    "\n",
    "  def __call__(self, inputs, training, weights, **kwargs):\n",
    "    return tf.keras.backend.clip(inputs, -1.0, 1.0)\n",
    "\n",
    "  def get_config(self):\n",
    "    # Not needed. No __init__ parameters to serialize.\n",
    "    return {}\n",
    "\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
    "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
    "      return [(layer.kernel, FixedRangeQuantizer())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "518685e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_11 (QuantizeL (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_47 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_75 (QuantizeWra (None, 30, 30, 12)        3279      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_55 (Quan (None, 15, 15, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_76 (QuantizeWra (None, 13, 13, 64)        7107      \n",
      "_________________________________________________________________\n",
      "quant_conv2d_77 (QuantizeWra (None, 11, 11, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_56 (Quan (None, 5, 5, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_78 (QuantizeWra (None, 3, 3, 256)         295683    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_57 (Quan (None, 1, 1, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_dense_48 (QuantizeWrap (None, 1, 1, 1028)        264201    \n",
      "_________________________________________________________________\n",
      "quant_flatten_36 (QuantizeWr (None, 1028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_49 (QuantizeWrap (None, 10)                10295     \n",
      "=================================================================\n",
      "Total params: 654,810\n",
      "Trainable params: 653,858\n",
      "Non-trainable params: 952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   \n",
    "    keras.layers.InputLayer( input_shape=(32, 32,3)),\n",
    "    #keras.layers.Reshape(target_shape=(32, 32, 3)),\n",
    "      # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "    quantize_annotate_layer(tf.keras.layers.Dense(30,), ModifiedDenseQuantizeConfig()),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64,kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(256,kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dense(1028, activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    \n",
    "    \n",
    "       \n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa65f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_11 (QuantizeL (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_47 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_75 (QuantizeWra (None, 30, 30, 12)        3279      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_55 (Quan (None, 15, 15, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_76 (QuantizeWra (None, 13, 13, 64)        7107      \n",
      "_________________________________________________________________\n",
      "quant_conv2d_77 (QuantizeWra (None, 11, 11, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_56 (Quan (None, 5, 5, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_78 (QuantizeWra (None, 3, 3, 256)         295683    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_57 (Quan (None, 1, 1, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_dense_48 (QuantizeWrap (None, 1, 1, 1028)        264201    \n",
      "_________________________________________________________________\n",
      "quant_flatten_36 (QuantizeWr (None, 1028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_49 (QuantizeWrap (None, 10)                10295     \n",
      "=================================================================\n",
      "Total params: 654,810\n",
      "Trainable params: 653,858\n",
      "Non-trainable params: 952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "571c573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 64s 165ms/step - loss: 1.4384 - accuracy: 0.4743\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 1.2055 - accuracy: 0.5700\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 63s 162ms/step - loss: 1.0695 - accuracy: 0.6239\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 0.9527 - accuracy: 0.6666\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 66s 168ms/step - loss: 0.8509 - accuracy: 0.7020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2207fb1a080>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.fit(x_train, y_train,  batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01ac9f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 3s - loss: 1.0360 - accuracy: 0.6530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.036031723022461, 0.652999997138977]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a659083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_10 (QuantizeL (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_43 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_71 (QuantizeWra (None, 30, 30, 256)       69891     \n",
      "_________________________________________________________________\n",
      "quant_conv2d_72 (QuantizeWra (None, 28, 28, 64)        147651    \n",
      "_________________________________________________________________\n",
      "quant_conv2d_73 (QuantizeWra (None, 26, 26, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_53 (Quan (None, 13, 13, 128)       1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_74 (QuantizeWra (None, 11, 11, 256)       295683    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_54 (Quan (None, 5, 5, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_34 (QuantizeWr (None, 6400)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_44 (QuantizeWrap (None, 512)               3277317   \n",
      "_________________________________________________________________\n",
      "quant_dropout_10 (QuantizeWr (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_35 (QuantizeWr (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_45 (QuantizeWrap (None, 1028)              527369    \n",
      "_________________________________________________________________\n",
      "quant_dropout_11 (QuantizeWr (None, 1028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_46 (QuantizeWrap (None, 10)                10295     \n",
      "=================================================================\n",
      "Total params: 4,402,453\n",
      "Trainable params: 4,401,006\n",
      "Non-trainable params: 1,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   \n",
    "             keras.layers.InputLayer( input_shape=(32, 32,3)),\n",
    "    #keras.layers.Reshape(target_shape=(32, 32, 3)),\n",
    "      # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "             quantize_annotate_layer(tf.keras.layers.Dense(30,), ModifiedDenseQuantizeConfig()),\n",
    "             tf.keras.layers.Conv2D(256,kernel_size=(3,3),activation=\"relu\"),\n",
    "             tf.keras.layers.Conv2D(64,kernel_size=(3,3),activation=\"relu\"),\n",
    "             tf.keras.layers.Conv2D(128,kernel_size=(3,3),activation=\"relu\"),\n",
    "             tf.keras.layers.MaxPooling2D(2,2),\n",
    "             tf.keras.layers.Conv2D(256,kernel_size=(3,3),activation=\"relu\"),\n",
    "             tf.keras.layers.MaxPooling2D(2,2),\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(512, activation=\"relu\"),        \n",
    "             tf.keras.layers.Dropout(0.5),\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(1028, activation=\"relu\"),            \n",
    "             tf.keras.layers.Dropout(0.5),\n",
    "             tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "     ]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3e762a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_10 (QuantizeL (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_43 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_71 (QuantizeWra (None, 30, 30, 256)       69891     \n",
      "_________________________________________________________________\n",
      "quant_conv2d_72 (QuantizeWra (None, 28, 28, 64)        147651    \n",
      "_________________________________________________________________\n",
      "quant_conv2d_73 (QuantizeWra (None, 26, 26, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_53 (Quan (None, 13, 13, 128)       1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_74 (QuantizeWra (None, 11, 11, 256)       295683    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_54 (Quan (None, 5, 5, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_34 (QuantizeWr (None, 6400)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_44 (QuantizeWrap (None, 512)               3277317   \n",
      "_________________________________________________________________\n",
      "quant_dropout_10 (QuantizeWr (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_35 (QuantizeWr (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_45 (QuantizeWrap (None, 1028)              527369    \n",
      "_________________________________________________________________\n",
      "quant_dropout_11 (QuantizeWr (None, 1028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_46 (QuantizeWrap (None, 10)                10295     \n",
      "=================================================================\n",
      "Total params: 4,402,453\n",
      "Trainable params: 4,401,006\n",
      "Non-trainable params: 1,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3732f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 456s 1s/step - loss: 0.6658 - accuracy: 0.7675\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 444s 1s/step - loss: 0.6009 - accuracy: 0.7886\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 439s 1s/step - loss: 0.5373 - accuracy: 0.8113\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 439s 1s/step - loss: 0.4837 - accuracy: 0.8295\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 456s 1s/step - loss: 0.4332 - accuracy: 0.8448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220813d3ba8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.fit(x_train, y_train,  batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af484f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 22s - loss: 0.7906 - accuracy: 0.7505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7906006574630737, 0.7505000233650208]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "786d4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_9 (QuantizeLa (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_41 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_67 (QuantizeWra (None, 30, 30, 12)        3279      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_50 (Quan (None, 15, 15, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_68 (QuantizeWra (None, 13, 13, 64)        7107      \n",
      "_________________________________________________________________\n",
      "quant_conv2d_69 (QuantizeWra (None, 11, 11, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_51 (Quan (None, 5, 5, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_70 (QuantizeWra (None, 3, 3, 256)         295683    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_52 (Quan (None, 1, 1, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_33 (QuantizeWr (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_42 (QuantizeWrap (None, 10)                2575      \n",
      "=================================================================\n",
      "Total params: 382,889\n",
      "Trainable params: 381,942\n",
      "Non-trainable params: 947\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   \n",
    "    keras.layers.InputLayer( input_shape=(32, 32,3)),\n",
    "    #keras.layers.Reshape(target_shape=(32, 32, 3)),\n",
    "      # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "    quantize_annotate_layer(tf.keras.layers.Dense(30,), ModifiedDenseQuantizeConfig()),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64,kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(256,kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    \n",
    "    \n",
    "       \n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1bb2c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_9 (QuantizeLa (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_41 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_67 (QuantizeWra (None, 30, 30, 12)        3279      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_50 (Quan (None, 15, 15, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_68 (QuantizeWra (None, 13, 13, 64)        7107      \n",
      "_________________________________________________________________\n",
      "quant_conv2d_69 (QuantizeWra (None, 11, 11, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_51 (Quan (None, 5, 5, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_70 (QuantizeWra (None, 3, 3, 256)         295683    \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_52 (Quan (None, 1, 1, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_33 (QuantizeWr (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_42 (QuantizeWrap (None, 10)                2575      \n",
      "=================================================================\n",
      "Total params: 382,889\n",
      "Trainable params: 381,942\n",
      "Non-trainable params: 947\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "104fce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 62s 159ms/step - loss: 0.8477 - accuracy: 0.7044\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 62s 159ms/step - loss: 0.7700 - accuracy: 0.7359\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 62s 160ms/step - loss: 0.6918 - accuracy: 0.7606\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 62s 160ms/step - loss: 0.6245 - accuracy: 0.7834\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 62s 159ms/step - loss: 0.5445 - accuracy: 0.8108\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 62s 159ms/step - loss: 0.4733 - accuracy: 0.8348\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 62s 159ms/step - loss: 0.4076 - accuracy: 0.8593\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 0.3473 - accuracy: 0.8789\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 0.2842 - accuracy: 0.9016\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 60s 154ms/step - loss: 0.2462 - accuracy: 0.9149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2207fe6bd30>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.fit(x_train, y_train,  batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0ecc7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 3s - loss: 1.4938 - accuracy: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4937769174575806, 0.6686000227928162]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14f10dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_8 (QuantizeLa (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_39 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_66 (QuantizeWra (None, 30, 30, 12)        3279      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_49 (Quan (None, 15, 15, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_32 (QuantizeWr (None, 2700)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_40 (QuantizeWrap (None, 10)                27015     \n",
      "=================================================================\n",
      "Total params: 30,422\n",
      "Trainable params: 30,382\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   \n",
    "    keras.layers.InputLayer( input_shape=(32, 32,3)),\n",
    "    #keras.layers.Reshape(target_shape=(32, 32, 3)),\n",
    "      # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "    quantize_annotate_layer(tf.keras.layers.Dense(30,), ModifiedDenseQuantizeConfig()),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    \n",
    "    \n",
    "       \n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2fc02cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_8 (QuantizeLa (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_dense_39 (QuantizeWrap (None, 32, 32, 30)        123       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_66 (QuantizeWra (None, 30, 30, 12)        3279      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_49 (Quan (None, 15, 15, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_32 (QuantizeWr (None, 2700)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_40 (QuantizeWrap (None, 10)                27015     \n",
      "=================================================================\n",
      "Total params: 30,422\n",
      "Trainable params: 30,382\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "270a75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.8931 - accuracy: 0.6911\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 0.8866 - accuracy: 0.6912\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 39s 99ms/step - loss: 0.8840 - accuracy: 0.6945\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.8767 - accuracy: 0.6950\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.8721 - accuracy: 0.6984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2207cf35e80>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.fit(x_train, y_train,  batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa54b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 1.1723 - accuracy: 0.6057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1723408699035645, 0.6057000160217285]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "437230a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa152315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 14s - loss: 0.6396 - accuracy: 0.7966\n",
      "313/313 - 3s - loss: 1.0360 - accuracy: 0.6530\n",
      "Baseline test accuracy: 0.7965999841690063\n",
      "Quantize test accuracy: 0.652999997138977\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = base_model.evaluate(\n",
    "    x_test, y_test, verbose=2)\n",
    "\n",
    "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
    "   x_test, y_test, verbose=2)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quantize test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "44537dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_47_layer_call_fn, dense_47_layer_call_and_return_conditional_losses, conv2d_75_layer_call_fn, conv2d_75_layer_call_and_return_conditional_losses, conv2d_76_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmpi22v8y8q\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmpi22v8y8q\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model20 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fde3a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f8eec64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.1\n",
      "Quant TF test accuracy: 0.652999997138977\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model20)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8012279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmppg8l6hua\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Windows\\TEMP\\tmppg8l6hua\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 35.54994201660156\n",
      "Quantized model in Mb: 1.4028549194335938\n"
     ]
    }
   ],
   "source": [
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
    "float_tflite_model21 = float_converter.convert()\n",
    "\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(quantized_tflite_model20)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model21)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735a007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
